{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Goal and Overview\n",
        "- This project involves sentiment analysis on Amazon product reviews using deep learning techniques. The objective is to classify product reviews into two categories: positive and negative sentiment. We will utilize the BERT (Bidirectional Encoder Representations from Transformers) model, a powerful pre-trained transformer model, to perform the sentiment classification.\n",
        "\n",
        "- last update date: Jan 3, 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Table of Contents\n",
        "1. [Importing Libraries and Preparing the Environment](amazon_sentiment.ipynb#1-importing-libraries-and-preparing-the-environment)\n",
        "2. [Downloading and Decompressing the Dataset](amazon_sentiment.ipynb#2-downloading-and-decompressing-the-dataset)\n",
        "3. [Previewing the Data](amazon_sentiment.ipynb#3-previewing-the-data)\n",
        "4. [Data Preprocessing](amazon_sentiment.ipynb#4-data-preprocessing)\n",
        "5. [Exploring Dataset Labels](amazon_sentiment.ipynb#5-exploring-dataset-labels)\n",
        "6. [Configuring the BERT Model](amazon_sentiment.ipynb#6-configuring-the-bert-model)\n",
        "7. [Fine-Tuning BERT for Sentiment Analysis](amazon_sentiment.ipynb#7-fine-tuning-bert-for-sentiment-analysis)\n",
        "8. [Evaluation on Test Data](amazon_sentiment.ipynb#8-evaluation-on-test-data)\n",
        "9. [Future Direction](amazon_sentiment.ipynb#9-future-direction)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M3HUJbqqP9I"
      },
      "source": [
        "## 1. Importing Libraries and Preparing the Environment\n",
        "In this step, we import the necessary Python libraries to build our sentiment classification model. These libraries include PyTorch, transformers, and others required for data processing and model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "waYGocejouhq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/QDAI8/opt/miniconda3/envs/dl/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import RandomSampler\n",
        "from tqdm import tqdm, trange\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import BertConfig, BertForSequenceClassification, BertTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Downloading and Decompressing the Dataset\n",
        "The dataset for this project was downloaded from Kaggle. It contains product reviews labeled with sentiment scores (positive or negative). We use the Kaggle API to download the dataset and then decompress it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data was downloaded from [Kaggle](https://www.kaggle.com/datasets/bittlingmayer/amazonreviews) and decompressed using following script:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "import kagglehub\n",
        "import shutil\n",
        "# Download the dataset\n",
        "path = kagglehub.dataset_download(\"bittlingmayer/amazonreviews\")\n",
        "shutil.move(path, '.')\n",
        "\n",
        "import bz2\n",
        "\n",
        "def decompress_bz2(file_path, output_path):\n",
        "    with bz2.open(file_path, 'rt', encoding='utf-8') as file:\n",
        "        with open(output_path, 'w', encoding='utf-8') as out_file:\n",
        "            out_file.write(file.read())\n",
        "\n",
        "# Decompress the files\n",
        "decompress_bz2('7/test.ft.txt.bz2', 'test.txt')\n",
        "decompress_bz2('7/train.ft.txt.bz2', 'train.txt')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Previewing the Data\n",
        "Here, we preview the first few lines of the train and test files to get a sense of the data format. Each line in the dataset contains a sentiment label and a product review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DFTSj8QxTir",
        "outputId": "c26aae97-eb33-4ea0-f142-4c62837c4a80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__label__2 Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\n",
            "\n",
            "__label__2 The best soundtrack ever to anything.: I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.\n",
            "\n",
            "__label__2 Amazing!: This soundtrack is my favorite music of all time, hands down. The intense sadness of \"Prisoners of Fate\" (which means all the more if you've played the game) and the hope in \"A Distant Promise\" and \"Girl who Stole the Star\" have been an important inspiration to me personally throughout my teen years. The higher energy tracks like \"Chrono Cross ~ Time's Scar~\", \"Time of the Dreamwatch\", and \"Chronomantique\" (indefinably remeniscent of Chrono Trigger) are all absolutely superb as well.This soundtrack is amazing music, probably the best of this composer's work (I haven't heard the Xenogears soundtrack, so I can't say for sure), and even if you've never played the game, it would be worth twice the price to buy it.I wish I could give it 6 stars.\n",
            "\n",
            "__label__2 Excellent Soundtrack: I truly like this soundtrack and I enjoy video game music. I have played this game and most of the music on here I enjoy and it's truly relaxing and peaceful.On disk one. my favorites are Scars Of Time, Between Life and Death, Forest Of Illusion, Fortress of Ancient Dragons, Lost Fragment, and Drowned Valley.Disk Two: The Draggons, Galdorb - Home, Chronomantique, Prisoners of Fate, Gale, and my girlfriend likes ZelbessDisk Three: The best of the three. Garden Of God, Chronopolis, Fates, Jellyfish sea, Burning Orphange, Dragon's Prayer, Tower Of Stars, Dragon God, and Radical Dreamers - Unstealable Jewel.Overall, this is a excellent soundtrack and should be brought by those that like video game music.Xander Cross\n",
            "\n",
            "__label__2 Remember, Pull Your Jaw Off The Floor After Hearing it: If you've played the game, you know how divine the music is! Every single song tells a story of the game, it's that good! The greatest songs are without a doubt, Chrono Cross: Time's Scar, Magical Dreamers: The Wind, The Stars, and the Sea and Radical Dreamers: Unstolen Jewel. (Translation varies) This music is perfect if you ask me, the best it can be. Yasunori Mitsuda just poured his heart on and wrote it down on paper.\n",
            "\n",
            "__label__2 Great CD: My lovely Pat has one of the GREAT voices of her generation. I have listened to this CD for YEARS and I still LOVE IT. When I'm in a good mood it makes me feel better. A bad mood just evaporates like sugar in the rain. This CD just oozes LIFE. Vocals are jusat STUUNNING and lyrics just kill. One of life's hidden gems. This is a desert isle CD in my book. Why she never made it big is just beyond me. Everytime I play this, no matter black, white, young, old, male, female EVERYBODY says one thing \"Who was that singing ?\"\n",
            "\n",
            "__label__2 One of the best game music soundtracks - for a game I didn't really play: Despite the fact that I have only played a small portion of the game, the music I heard (plus the connection to Chrono Trigger which was great as well) led me to purchase the soundtrack, and it remains one of my favorite albums. There is an incredible mix of fun, epic, and emotional songs. Those sad and beautiful tracks I especially like, as there's not too many of those kinds of songs in my other video game soundtracks. I must admit that one of the songs (Life-A Distant Promise) has brought tears to my eyes on many occasions.My one complaint about this soundtrack is that they use guitar fretting effects in many of the songs, which I find distracting. But even if those weren't included I would still consider the collection worth it.\n",
            "\n",
            "__label__1 Batteries died within a year ...: I bought this charger in Jul 2003 and it worked OK for a while. The design is nice and convenient. However, after about a year, the batteries would not hold a charge. Might as well just get alkaline disposables, or look elsewhere for a charger that comes with batteries that have better staying power.\n",
            "\n",
            "__label__2 works fine, but Maha Energy is better: Check out Maha Energy's website. Their Powerex MH-C204F charger works in 100 minutes for rapid charge, with option for slower charge (better for batteries). And they have 2200 mAh batteries.\n",
            "\n",
            "__label__2 Great for the non-audiophile: Reviewed quite a bit of the combo players and was hesitant due to unfavorable reviews and size of machines. I am weaning off my VHS collection, but don't want to replace them with DVD's. This unit is well built, easy to setup and resolution and special effects (no progressive scan for HDTV owners) suitable for many people looking for a versatile product.Cons- No universal remote.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Preview the first few lines of the train file\n",
        "with open('train.txt', 'r') as train_file:\n",
        "    for _ in range(5):  # Print the first 5 lines\n",
        "        print(train_file.readline())\n",
        "\n",
        "# Preview the first few lines of the test file\n",
        "with open('test.txt', 'r') as test_file:\n",
        "    for _ in range(5):  # Print the first 5 lines\n",
        "        print(test_file.readline())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Preprocessing\n",
        "Before feeding the data into the model, we need to preprocess the text (e.g., tokenization and cleaning) so that the BERT model can properly interpret it. This step will involve converting text into token IDs and padding them to the correct length for input to the BERT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aPjX2jMFubH4"
      },
      "outputs": [],
      "source": [
        "# Function to pad sequences\n",
        "def rpad(array, n):\n",
        "    current_len = len(array)\n",
        "    if current_len > n:\n",
        "        return array[:n]\n",
        "    extra = n - current_len\n",
        "    return array + ([0] * extra)\n",
        "\n",
        "# Parse line and extract label and text\n",
        "def parse_line_with_label(line):\n",
        "    line = line.strip().lower()\n",
        "    line = line.replace(\"&nbsp;\", \" \")\n",
        "    line = re.sub(r'<br(\\s\\/)?>', ' ', line)\n",
        "    line = re.sub(r' +', ' ', line)  # Merge multiple spaces into one\n",
        "\n",
        "    # Extract label and text\n",
        "    match = re.match(r'__label__(\\d+)\\s(.+)', line)\n",
        "    if match:\n",
        "        label = int(match.group(1))  # Extract label (e.g., 2)\n",
        "        text = match.group(2)       # Extract text after the label\n",
        "        return text, label\n",
        "    return None, None\n",
        "\n",
        "# Read dataset and parse each line\n",
        "def read_labeled_data(filename):\n",
        "    data = []\n",
        "    labels = []\n",
        "    with open(filename, 'r', encoding=\"utf-8\") as file:\n",
        "        for line in file:\n",
        "            text, label = parse_line_with_label(line)\n",
        "            if text and label is not None:\n",
        "                label = label - 1\n",
        "                data.append(text)\n",
        "                labels.append(label)\n",
        "    return data, labels\n",
        "\n",
        "# Tokenizer and embedding conversion\n",
        "def convert_to_embedding(tokenizer, sentences_with_labels):\n",
        "    for sentence, label in sentences_with_labels:\n",
        "        tokens = tokenizer.tokenize(sentence)\n",
        "        tokens = tokens[:250]\n",
        "        bert_sent = rpad(tokenizer.convert_tokens_to_ids([\"CLS\"] + tokens + [\"SEP\"]), n=256)\n",
        "        yield torch.tensor(bert_sent), torch.tensor(label, dtype=torch.int64)\n",
        "\n",
        "# Prepare the dataloader\n",
        "def prepare_dataloader(tokenizer, sampler=RandomSampler, train=False):\n",
        "    filename = 'sample_train.txt' if train else 'sample_test.txt'\n",
        "\n",
        "    data, labels = read_labeled_data(filename)\n",
        "    sentences_with_labels = zip(data, labels)\n",
        "\n",
        "    dataset = list(convert_to_embedding(tokenizer, sentences_with_labels))\n",
        "\n",
        "    sampler_func = sampler(dataset) if sampler is not None else None\n",
        "    dataloader = DataLoader(dataset, sampler=sampler_func, batch_size=32)  # Set your batch size here\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Exploring Dataset Labels\n",
        "Before training, we examine the distribution of labels in the dataset. This helps ensure the dataset is balanced and provides insights into the classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hzOddwH-e_I",
        "outputId": "90a248f5-162c-4dcf-9603-435a0cf81c86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train label counts: Counter({1: 1800000, 0: 1800000})\n",
            "Test label counts: Counter({1: 200000, 0: 200000})\n"
          ]
        }
      ],
      "source": [
        "# Read the train and test data\n",
        "train_data, train_labels = read_labeled_data(\"train.txt\")\n",
        "test_data, test_labels = read_labeled_data(\"test.txt\")\n",
        "\n",
        "# Count the occurrences of each label\n",
        "from collections import Counter\n",
        "\n",
        "train_label_counts = Counter(train_labels)\n",
        "test_label_counts = Counter(test_labels)\n",
        "\n",
        "print(\"Train label counts:\", train_label_counts)\n",
        "print(\"Test label counts:\", test_label_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To make the project more computationally feasible, we sampled the original dataset. This approach ensures that the model can be trained efficiently while still achieving reasonable performance. The sampled dataset contains:\n",
        "- Train Dataset: 1800 samples\n",
        "- Test Dataset: 200 samples \n",
        "\n",
        "The sampling was done randomly to maintain the original label distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def create_sample_file(input_file, output_file, sample_size=100):\n",
        "    \"\"\"\n",
        "    Create a sample file from the input data.\n",
        "\n",
        "    Args:\n",
        "        input_file (str): Path to the original dataset file.\n",
        "        output_file (str): Path to the output sample file.\n",
        "        sample_size (int): Number of lines to sample.\n",
        "    \"\"\"\n",
        "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
        "        lines = infile.readlines()\n",
        "\n",
        "    sample = random.sample(lines, min(sample_size, len(lines)))\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "        outfile.writelines(sample)\n",
        "\n",
        "# Generate sample training and testing files\n",
        "create_sample_file('train.txt', 'sample_train.txt', sample_size=1800)\n",
        "create_sample_file('test.txt', 'sample_test.txt', sample_size=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After sampling, it's important to confirm that the label distribution remains similar to the original dataset to ensure balanced training. Below are the label counts for the sampled data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train label counts: Counter({1: 934, 0: 866})\n",
            "Test label counts: Counter({1: 101, 0: 99})\n"
          ]
        }
      ],
      "source": [
        "# Read the train and test data\n",
        "train_data, train_labels = read_labeled_data(\"sample_train.txt\")\n",
        "test_data, test_labels = read_labeled_data(\"sample_test.txt\")\n",
        "\n",
        "# Count the occurrences of each label\n",
        "from collections import Counter\n",
        "\n",
        "train_label_counts = Counter(train_labels)\n",
        "test_label_counts = Counter(test_labels)\n",
        "\n",
        "print(\"Train label counts:\", train_label_counts)\n",
        "print(\"Test label counts:\", test_label_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Configuring the BERT Model\n",
        "We initialize the hyperparameters and configurations for fine-tuning the BERT model. This includes batch size, learning rates, warm-up steps, and other critical parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "PAD_TOKEN_LABEL_ID = CrossEntropyLoss().ignore_index\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE_MODEL = 1e-5\n",
        "LEARNING_RATE_CLASSIFIER = 1e-3\n",
        "WARMUP_STEPS = 0\n",
        "GRADIENT_ACCUMULATION_STEPS = 1\n",
        "MAX_GRAD_NORM = 1.0\n",
        "SEED = 42\n",
        "NO_CUDA = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Fine-Tuning BERT for Sentiment Analysis\n",
        "In this section, we define the process for training the BERT model on the sentiment analysis task. The model will be fine-tuned using the preprocessed Amazon reviews dataset. The training loop includes loss calculation, backpropagation, and gradient clipping to ensure stable training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Transformers:\n",
        "    model = None\n",
        "\n",
        "    def __init__(self, tokenizer):\n",
        "        self.pad_token_label_id = PAD_TOKEN_LABEL_ID\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and not NO_CUDA else \"cpu\")\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def predict(self, sentence):\n",
        "        if self.model is None or self.tokenizer is None:\n",
        "            self.load()\n",
        "\n",
        "        embeddings = list(convert_to_embedding([(sentence, -1)]))\n",
        "        preds = self._predict_tags_batched(embeddings)\n",
        "        return preds\n",
        "\n",
        "    def evaluate(self, dataloader):\n",
        "        from sklearn.metrics import classification_report\n",
        "        y_pred = self._predict_tags_batched(dataloader)\n",
        "        # y_true = np.append(np.zeros(50), np.ones(50))\n",
        "        y_true = []\n",
        "        for _, labels in dataloader:\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "        score = classification_report(y_true, y_pred)\n",
        "        return score\n",
        "\n",
        "    def _predict_tags_batched(self, dataloader):\n",
        "        preds = []\n",
        "        self.model.eval()\n",
        "        for batch in tqdm(dataloader, desc=\"Computing NER tags\"):\n",
        "            batch = tuple(t.to(self.device) for t in batch)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(batch[0])\n",
        "                _, is_neg = torch.max(outputs[0], 1)\n",
        "                preds.extend(is_neg.cpu().detach().numpy())\n",
        "\n",
        "        return preds\n",
        "\n",
        "    def train(self, dataloader, model, epochs):\n",
        "        assert self.model is None  # make sure we are not training after load() command\n",
        "        model.to(self.device)\n",
        "        self.model = model\n",
        "\n",
        "        t_total = len(dataloader) // GRADIENT_ACCUMULATION_STEPS * epochs\n",
        "        # Number of iteractions\n",
        "\n",
        "        # Prepare optimizer and schedule (linear warmup and decay)\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\"params\": model.bert.parameters(), \"lr\": LEARNING_RATE_MODEL},\n",
        "            {\"params\": model.classifier.parameters(), \"lr\": LEARNING_RATE_CLASSIFIER}\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters)\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=t_total)\n",
        "\n",
        "        # Train!\n",
        "        print(\"***** Running training *****\")\n",
        "        print(\"Training on %d examples\" % len(dataloader))\n",
        "        print(\"Num Epochs = %d\" % epochs)\n",
        "        print(\"Total optimization steps = %d\" % t_total)\n",
        "        \n",
        "        global_step = 0\n",
        "        tr_loss, logging_loss = 0.0, 0.0\n",
        "        model.zero_grad()\n",
        "        train_iterator = trange(epochs, desc=\"Epoch\")\n",
        "        self._set_seed()\n",
        "        for _ in train_iterator:\n",
        "            epoch_iterator = tqdm(dataloader, desc=\"Iteration\")\n",
        "            for step, batch in enumerate(epoch_iterator):\n",
        "                model.train()\n",
        "                batch = tuple(t.to(self.device) for t in batch)\n",
        "                outputs = model(batch[0], labels=batch[1])\n",
        "                loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n",
        "\n",
        "                if GRADIENT_ACCUMULATION_STEPS > 1:\n",
        "                    loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                tr_loss += loss.item()\n",
        "                if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
        "                    scheduler.step()  # Update learning rate schedule\n",
        "                    optimizer.step()\n",
        "                    model.zero_grad()\n",
        "                    global_step += 1\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "        return global_step, tr_loss / global_step\n",
        "\n",
        "    def _set_seed(self):\n",
        "        torch.manual_seed(SEED)\n",
        "        if self.device == 'gpu':\n",
        "            torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "    def load(self, model_dir='weights/'):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
        "        self.model = BertForSequenceClassification.from_pretrained(model_dir)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "def train(epochs=20, output_dir=\"weights/\"):\n",
        "    num_labels = 2  # negative and positive reviews\n",
        "    config = BertConfig.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
        "\n",
        "    dataloader = prepare_dataloader(tokenizer, train=True)\n",
        "    predictor = Transformers(tokenizer)\n",
        "    predictor.train(dataloader, model, epochs)\n",
        "\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "def evaluate(model_dir=\"weights/\"):\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "    dataloader = prepare_dataloader(tokenizer, train=False, sampler=None)\n",
        "    predictor = Transformers(tokenizer)\n",
        "    predictor.load(model_dir=model_dir)\n",
        "    out = predictor.evaluate(dataloader)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Train with 3 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "y5d1Y1n83P9x",
        "outputId": "f1c3e9d5-291e-4ed8-efeb-fe4765cd7ba1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/Users/QDAI8/opt/miniconda3/envs/dl/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "Training on 57 examples\n",
            "Num Epochs = 3\n",
            "Total optimization steps = 171\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
            "/Users/QDAI8/opt/miniconda3/envs/dl/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "Iteration: 100%|██████████| 57/57 [52:25<00:00, 55.19s/it]\n",
            "Iteration: 100%|██████████| 57/57 [50:37<00:00, 53.28s/it]\n",
            "Iteration: 100%|██████████| 57/57 [47:24<00:00, 49.91s/it]\n",
            "Epoch: 100%|██████████| 3/3 [2:30:27<00:00, 3009.29s/it]\n"
          ]
        }
      ],
      "source": [
        "path = './'\n",
        "os.makedirs(path, exist_ok=True)\n",
        "train(epochs=3, output_dir=path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluation on Test Data\n",
        "After training, we evaluate the model's performance on the test dataset to measure its effectiveness. Metrics such as accuracy and F1-score are used to assess the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAwvHKUA9lHz",
        "outputId": "1bc2b2a8-c665-4381-ebb3-71b234e84b4d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing NER tags: 100%|██████████| 7/7 [01:37<00:00, 13.97s/it]\n"
          ]
        }
      ],
      "source": [
        "out = evaluate(model_dir=path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjC8Y5BH97sR",
        "outputId": "506856a8-1b0e-497a-fe92-233715252660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.80      0.81        99\n",
            "           1       0.81      0.84      0.83       101\n",
            "\n",
            "    accuracy                           0.82       200\n",
            "   macro avg       0.82      0.82      0.82       200\n",
            "weighted avg       0.82      0.82      0.82       200\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Future Direction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.1 Model Optimization\n",
        "\n",
        "- Experiment with Alternative Transformer Architectures: Explore models like RoBERTa, XLNet, or DistilBERT to compare performance, efficiency, and suitability for the sentiment classification task.\n",
        "\n",
        "- Hyperparameter Tuning: Optimize parameters like batch size, learning rate, number of epochs, and more using automated tools such as:\n",
        "    - Optuna: A hyperparameter optimization library for efficient searches.\n",
        "    - Ray Tune: A scalable framework for distributed hyperparameter tuning.\n",
        "\n",
        "- Gradient Accumulation: Simulate larger batch sizes on memory-limited devices (e.g., CPUs) by accumulating gradients over multiple steps before performing an update (GRADIENT_ACCUMULATION_STEPS > 1).\n",
        "\n",
        "- Learning Rate Optimization\n",
        "    - Adjust learning rates dynamically during training for better convergence using learning rate schedules:\n",
        "        - StepLR: Decreases the learning rate by a fixed factor every few epochs.\n",
        "        - ExponentialLR: Exponentially reduces the learning rate.\n",
        "        - ReduceLROnPlateau: Decreases the learning rate when the monitored metric stops improving.\n",
        "    - Learning Rate Warmup: Gradually increase the learning rate during initial training steps to stabilize optimization and prevent large parameter updates early on. Experiment with non-zero WARMUP_STEPS (e.g., 10% of the total steps).\n",
        "\n",
        "- Mixed Precision Training: Use mixed precision (combining 16-bit and 32-bit floating point operations) to speed up training and reduce memory consumption."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.2 Multi-Class Sentiment Analysis\n",
        "\n",
        "Extend the project to handle multi-class sentiment analysis (e.g., \"positive,\" \"neutral,\" \"negative\") rather than binary classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.3 Scalability\n",
        "\n",
        "- Implement distributed training using tools like PyTorch's DistributedDataParallel or Hugging Face Accelerate to handle larger datasets efficiently.\n",
        "- Use libraries like Dask or Apache Spark for parallelized data preprocessing."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
